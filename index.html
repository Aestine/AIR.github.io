<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A.I.R.: Adaptive, Iterative, and Reasoning-based Frame Selection for Video Question Answering">
  <meta name="keywords" content="Video QA, Frame Selection, Vision-Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A.I.R.: Adaptive, Iterative, and Reasoning-based Frame Selection for Video Question Answering</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A.I.R.: Adaptive, Iterative, and Reasoning-based Frame Selection for Video Question Answering</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous authors</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Paper under double-blind review at ICLR 2026</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/detail_pipeline.png" alt="A.I.R. Framework Overview">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">A.I.R.</span> addresses the critical trade-off between lightweight models' poor performance on complex queries 
        and VLM-based methods' prohibitive computational costs through adaptive, iterative frame selection.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Effectively applying Vision-Language Models (VLMs) to Video Question Answering (VideoQA) hinges on selecting a concise yet comprehensive set of frames,
            as processing entire videos is computationally infeasible. However, current frame selection methods face a critical trade-off: approaches relying on lightweight 
            similarity models, such as CLIP, often fail to capture the nuances of complex queries, resulting in inaccurate similarity scores that cannot accurately reflect 
            the authentic query-frame relevance, which further undermines frame selection.
          </p>
          <p>
            Meanwhile, methods that leverage a VLM for deeper analysis achieve higher accuracy but incur prohibitive computational costs. 
            To address these limitations, we propose A.I.R, a training-free approach for Adaptive, Iterative, and Reasoning-based frame selection. 
            We leverage a powerful VLM to perform deep, semantic analysis on complex queries, and this analysis is deployed within a cost-effective 
            iterative loop that processes only a small batch of the most promising frames at a time.
          </p>
          <p>
            Extensive experiments on various VideoQA benchmarks demonstrate that our approach outperforms existing frame selection methods, 
            significantly boosts the performance of the foundation VLM, and achieves substantial gains in computational efficiency over other VLM-based techniques.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            A.I.R. operates in two main stages:
          </p>
          <ol>
            <li><strong>Adaptive Initial Sampling:</strong> Identifies query-relevant events based on the unique distribution of query-frame similarity scores in each video, 
            sampling frames proportionally to event duration.</li>
            <li><strong>Iterative Frame Selection:</strong> Executes an efficient iterative loop that adaptively allocates VLM computation through four steps:
              <ul>
                <li>Interval Potential Ranking</li>
                <li>Reasoning-based VLM Analysis</li>
                <li>Early Stop Mechanism</li>
                <li>Localized Density Sampling</li>
              </ul>
            </li>
          </ol>
        </div>
      </div>
    </div>
    <!--/ Method Overview. -->

    <!-- Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            Our method demonstrates significant improvements across multiple VideoQA benchmarks:
          </p>
        </div>
        
        <!-- Add results table or figures here -->
        <table class="table is-bordered is-striped is-narrow is-hoverable">
          <thead>
            <tr>
              <th>Model</th>
              <th>Video-MME (w/o sub)</th>
              <th>MLVU</th>
              <th>LVB</th>
              <th>EgoSchema</th>
              <th>NextQA</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>QwenVL-2.5 (baseline)</td>
              <td>60.8</td>
              <td>59.3</td>
              <td>58.1</td>
              <td>57.6</td>
              <td>74.3</td>
            </tr>
            <tr>
              <td><strong>QwenVL-2.5 + A.I.R.</strong></td>
              <td><strong>65.0</strong></td>
              <td><strong>67.5</strong></td>
              <td><strong>61.4</strong></td>
              <td><strong>58.8</strong></td>
              <td><strong>81.3</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
    <!--/ Results -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Efficiency Analysis -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Efficiency Analysis</h2>
        <div class="content has-text-justified">
          <p>
            A.I.R. achieves superior computational efficiency by adaptively processing frames based on query complexity. 
            While conventional VLM-based methods analyze a fixed 128 frames (taking ~162 seconds), A.I.R. analyzes 
            only 36.5 frames on average (42.31 seconds), achieving better accuracy with 74% fewer frame analyses.
          </p>
        </div>
      </div>
    </div>
    <!--/ Efficiency Analysis -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{anonymous2026air,
  author    = {Anonymous},
  title     = {A.I.R.: Adaptive, Iterative, and Reasoning-based Frame Selection for Video Question Answering},
  journal   = {Under review at ICLR},
  year      = {2026},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
